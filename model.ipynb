{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, accuracy_score, f1_score\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    21007\n",
      "1     1890\n",
      "Name: merchant, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv('data/processed/merchant_data.csv')\n",
    "print(df.groupby('label')['merchant'].count()) \n",
    "\n",
    "# fill active days nan with 0\n",
    "df['active_days'] = df['active_days'].fillna(0)\n",
    "# fill average days beteen charges nan with  65\n",
    "df['avg_days_between'] = df['avg_days_between'].fillna(65)\n",
    "\n",
    "# Define your feature matrix X and target vector y\n",
    "X = df.drop(columns=['merchant', 'label', 'first_charge_date', 'first_payment_date'])  # drop non-feature columns\n",
    "# drop columns whose column name contains '_mean','_std', '_cv'\n",
    "\n",
    "y = df['label']\n",
    "all_variable = X.columns.tolist()\n",
    "categorical_vars = ['industry', 'country', 'business_size']\n",
    "continuous_vars = [i for i in all_variable if i not in categorical_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing and normalization\n",
    "# Imputer for continuous variables\n",
    "continuous_imputer = SimpleImputer(strategy='constant', fill_value=0)  \n",
    "# Imputer for categorical variables\n",
    "categorical_imputer = SimpleImputer(strategy='constant', fill_value='Missing')  # or 'most_frequent'\n",
    "\n",
    "# Define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', continuous_imputer),  # First impute missing values\n",
    "            ('scaler', StandardScaler())  # Then scale\n",
    "        ]), continuous_vars),\n",
    "        \n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', categorical_imputer),  # First impute missing values\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))  # Then encode\n",
    "        ]), categorical_vars)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "# it is highly imbalance, so we need to stratify the split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(X_train)\n",
    "# Transform both training and test data\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "# }\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 4, 10, 1),  # Adjusted for potential underfitting\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.001, 0.1),  # Lower end for more models\n",
    "    'num_boost_round': hp.quniform('num_boost_round', 100, 600, 50),  # More rounds\n",
    "    'gamma': hp.uniform('gamma', 0.0, 0.5),  # Fine as is\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),  # Fine as is\n",
    "    'subsample': hp.uniform('subsample', 0.6, 1),  # Adjusted range\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1),  # Adjusted range\n",
    "    'scale_pos_weight': scale_pos_weight  # Fine as is\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params_list = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'gamma': params['gamma'],\n",
    "        'min_child_weight': int(params['min_child_weight']),\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'objective': 'binary:logistic',\n",
    "        'scale_pos_weight': params['scale_pos_weight']\n",
    "    }\n",
    "    num_boost_round = int(params['num_boost_round'])\n",
    "    # Initialize an empty list to hold AUC scores\n",
    "    val_auc_scores = []\n",
    "    train_auc_scores = []\n",
    "    \n",
    "    # Create a StratifiedKFold object\n",
    "    kf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train_transformed, y_train):\n",
    "        X_train_k, X_val_k = X_train_transformed[train_index], X_train_transformed[val_index]\n",
    "        y_train_k, y_val_k = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        # Convert the dataset into an optimized data structure called Dmatrix that XGBoost supports\n",
    "        dtrain = xgb.DMatrix(X_train_k, label=y_train_k)\n",
    "        dval = xgb.DMatrix(X_val_k, label=y_val_k)\n",
    "\n",
    "        # Train the model with early stopping\n",
    "        evals = [(dtrain, 'train'), (dval, 'eval')]\n",
    "        model = xgb.train(params_list, dtrain, evals=evals, num_boost_round = num_boost_round, early_stopping_rounds=100, verbose_eval=False)\n",
    "        \n",
    "        # Predict on the validation set using the best iteration\n",
    "        best_iteration = model.best_iteration\n",
    "        preds_val_k = model.predict(dval, iteration_range=(0, best_iteration))\n",
    "        preds_train_k = model.predict(dtrain, iteration_range=(0, best_iteration))\n",
    "        \n",
    "        # Calculate and append the performance metric on the validation set\n",
    "        auc_val_k = roc_auc_score(y_val_k, preds_val_k)\n",
    "        val_auc_scores.append(auc_val_k)\n",
    "        auc_train_k = roc_auc_score(y_train_k, preds_train_k)\n",
    "        train_auc_scores.append(auc_train_k)\n",
    "    # Calculate the average AUC score across all folds\n",
    "    avg_auc = np.mean(val_auc_scores)\n",
    "    avg_auc_train = np.mean(train_auc_scores)\n",
    "    #print('Train AUC: {:.5f}, Val AUC: {:.5f}'.format(avg_auc_train, avg_auc))\n",
    "    return {'loss': -avg_auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [05:14<00:00,  3.14s/trial, best loss: -0.8633913134314556]\n"
     ]
    }
   ],
   "source": [
    "# Run hyperparameter tuning\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert hyperparameters to appropriate types\n",
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['min_child_weight'] = int(best['min_child_weight'])\n",
    "best['num_boost_round'] = int(best['num_boost_round'])\n",
    "best['objective'] = 'binary:logistic'\n",
    "best['scale_pos_weight'] = scale_pos_weight\n",
    "best['eval_metric'] = 'auc'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Placebo/anaconda3/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:742: \n",
      "Parameters: { \"num_boost_round\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final ROC AUC on the test set is: 0.8642809912049817\n",
      "f1 score:0.46586345381526106\n",
      "threshold:0.7200000000000001\n",
      "Recall on test set: 0.4702702702702703\n",
      "Precision on test set: 0.46153846153846156\n",
      "Accuracy on test set: 0.912882096069869\n",
      "F1 score on test set: 0.46586345381526106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Placebo/anaconda3/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [23:46:31] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for XGBoost format\n",
    "dtrain_full = xgb.DMatrix(X_train_transformed, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_transformed)\n",
    "# Train the final model on the full training data\n",
    "# num_boost_round = best.pop('num_boost_round')\n",
    "final_model = xgb.train(best, dtrain_full, num_boost_round=best['num_boost_round'])\n",
    "# save model\n",
    "final_model.save_model('model/xgb.model')\n",
    "# Predict on the test set\n",
    "final_preds = final_model.predict(dtest)\n",
    "\n",
    "# Calculate the final performance metric on the test set\n",
    "final_auc = roc_auc_score(y_test, final_preds)\n",
    "print(f\"The final ROC AUC on the test set is: {final_auc}\")\n",
    "\n",
    "# based on the best F1 score find the best threshold\n",
    "thresholds = np.arange(0.1, 1, 0.02)\n",
    "f1_scores = []\n",
    "for thresh in thresholds:\n",
    "    y_pred_binary = [1 if prob > thresh else 0 for prob in final_preds]\n",
    "    f1_scores.append(f1_score(y_test, y_pred_binary))\n",
    "\n",
    "print('f1 score:' + str(np.max(f1_scores)))\n",
    "threshold = thresholds[np.argmax(f1_scores)]\n",
    "print('threshold:' + str(threshold))\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold (e.g., 0.5)\n",
    "y_pred_binary = [1 if prob > threshold else 0 for prob in final_preds]\n",
    "\n",
    "# Calculate Recall\n",
    "test_recall = recall_score(y_test, y_pred_binary)\n",
    "print(f\"Recall on test set: {test_recall}\")\n",
    "\n",
    "# Calculate Precision\n",
    "test_precision = precision_score(y_test, y_pred_binary)\n",
    "print(f\"Precision on test set: {test_precision}\")\n",
    "\n",
    "# Calculate Accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "\n",
    "# Calculate F1 score\n",
    "test_f1 = f1_score(y_test, y_pred_binary)\n",
    "print(f\"F1 score on test set: {test_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21007, 30)\n",
      "(976, 2)\n"
     ]
    }
   ],
   "source": [
    "# Propensity to use Subscription on the data with label 0\n",
    "X_no_subscription = X[y == 0]\n",
    "print(X_no_subscription.shape)\n",
    "X_no_subscription_transformed = preprocessor.transform(X_no_subscription)\n",
    "d_no_subscription = xgb.DMatrix(X_no_subscription_transformed)\n",
    "preds_no_subscription = final_model.predict(d_no_subscription)\n",
    "\n",
    "# combine with merchant name \n",
    "merchant_name = df[y == 0]['merchant']\n",
    "merchant_name = merchant_name.reset_index(drop=True)\n",
    "preds_no_subscription = pd.DataFrame(preds_no_subscription, columns=['propensity'])\n",
    "preds_no_subscription = pd.concat([merchant_name, preds_no_subscription], axis=1)\n",
    "preds_no_subscription = preds_no_subscription.sort_values(by=['propensity'], ascending=False)\n",
    "preds_no_subscription.to_csv('data/result/merchant_propensity_subscription.csv', index=False)\n",
    "\n",
    "# select the merchant with propensity > threshold\n",
    "merchant_selected = preds_no_subscription[preds_no_subscription['propensity'] > threshold]\n",
    "print(merchant_selected.shape)\n",
    "merchant_selected.to_csv('data/result/merchant_selected.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
